{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/covertype_training_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/covertype_training_pipeline.py\n",
    "\n",
    "\"\"\"KFP orchestrating BigQuery and Cloud AI Platform services.\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from helper_components import evaluate_model\n",
    "from helper_components import retrieve_best_run\n",
    "from helper_components import prepoc_split_dataset\n",
    "from jinja2 import Template\n",
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp.dsl.types import Dict\n",
    "from kfp.dsl.types import GCPProjectID\n",
    "from kfp.dsl.types import GCPRegion\n",
    "from kfp.dsl.types import GCSPath\n",
    "from kfp.dsl.types import String\n",
    "from kfp.gcp import use_gcp_secret\n",
    "\n",
    "# Defaults and environment settings\n",
    "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
    "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
    "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
    "PYTHON_VERSION = os.getenv('PYTHON_VERSION')\n",
    "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
    "USE_KFP_SA = os.getenv('USE_KFP_SA')\n",
    "\n",
    "TRAINING_FILE_PATH = 'datasets/training/data.csv'\n",
    "VALIDATION_FILE_PATH = 'datasets/validation/data.csv'\n",
    "TESTING_FILE_PATH = 'datasets/testing/data.csv'\n",
    "\n",
    "# Parameter defaults\n",
    "SPLITS_DATASET_ID = 'splits'\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 6,\n",
    "        \"maxParallelTrials\": 3,\n",
    "        \"hyperparameterMetricTag\": \"accuracy\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"max_iter\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [500, 1000]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"alpha\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.0001,\n",
    "                \"maxValue\": 0.001,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "prepoc_split_op = func_to_container_op(prepoc_split_dataset, base_image=BASE_IMAGE)\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')\n",
    "retrieve_best_run_op = func_to_container_op(\n",
    "    retrieve_best_run, base_image=BASE_IMAGE)\n",
    "evaluate_model_op = func_to_container_op(evaluate_model, base_image=BASE_IMAGE)\n",
    "\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name='Covertype Classifier Training',\n",
    "    description='The pipeline training and deploying the Covertype classifierpipeline_yaml'\n",
    ")\n",
    "def covertype_train(project_id,\n",
    "                    region,\n",
    "                    source_table_name,\n",
    "                    gcs_root,\n",
    "                    dataset_id,\n",
    "                    evaluation_metric_name,\n",
    "                    evaluation_metric_threshold,\n",
    "                    model_id,\n",
    "                    version_id,\n",
    "                    replace_existing_version,\n",
    "                    hypertune_settings=HYPERTUNE_SETTINGS,\n",
    "                    dataset_location='US'):\n",
    "    \"\"\"Orchestrates training and deployment of an sklearn model.\"\"\"\n",
    "\n",
    "    splited_prepoc_dataset = prepoc_split_op(gcs_root)\n",
    "   \n",
    "    # Tune hyperparameters\n",
    "    tune_args = [\n",
    "        '--training_dataset_path',\n",
    "        splited_prepoc_dataset.outputs['training_file_path'],\n",
    "        '--validation_dataset_path',\n",
    "        splited_prepoc_dataset.outputs['validation_file_path'], '--hptune', 'True'\n",
    "    ]\n",
    "\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "                                kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    hypertune = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAINER_IMAGE,\n",
    "        job_dir=job_dir,\n",
    "        args=tune_args,\n",
    "        training_input=hypertune_settings)\n",
    "\n",
    "    # Retrieve the best trial\n",
    "    get_best_trial = retrieve_best_run_op(\n",
    "            project_id, hypertune.outputs['job_id'])\n",
    "\n",
    "    # Train the model on a combined training and validation datasets\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    train_args = [\n",
    "        '--training_dataset_path',\n",
    "        splited_prepoc_dataset.outputs['training_file_path'],\n",
    "        '--validation_dataset_path',\n",
    "        splited_prepoc_dataset.outputs['validation_file_path'], '--alpha',\n",
    "        get_best_trial.outputs['alpha'], '--max_iter',\n",
    "        get_best_trial.outputs['max_iter'], '--hptune', 'False'\n",
    "    ]\n",
    "\n",
    "    train_model = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAINER_IMAGE,\n",
    "        job_dir=job_dir,\n",
    "        args=train_args)\n",
    "\n",
    "    # Evaluate the model on the testing split\n",
    "    eval_model = evaluate_model_op(\n",
    "        dataset_path=str(splited_prepoc_dataset.outputs['testing_file_path']),\n",
    "        model_path=str(train_model.outputs['job_dir']),\n",
    "        metric_name=evaluation_metric_name)\n",
    "\n",
    "    # Deploy the model if the primary metric is better than threshold\n",
    "    with kfp.dsl.Condition(eval_model.outputs['metric_value'] > evaluation_metric_threshold):\n",
    "        deploy_model = mlengine_deploy_op(\n",
    "        model_uri=train_model.outputs['job_dir'],\n",
    "        project_id=project_id,\n",
    "        model_id=model_id,\n",
    "        version_id=version_id,\n",
    "        runtime_version=RUNTIME_VERSION,\n",
    "        python_version=PYTHON_VERSION,\n",
    "        replace_existing_version=replace_existing_version)\n",
    "\n",
    "    # Configure the pipeline to run using the service account defined\n",
    "    # in the user-gcp-sa k8s secret\n",
    "    if USE_KFP_SA == 'True':\n",
    "        kfp.dsl.get_pipeline_conf().add_op_transformer(\n",
    "              use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://artifacts.qwiklabs-gcp-01-9fd50757f427.appspot.com/\n",
      "gs://qwiklabs-gcp-01-9fd50757f427-kubeflowpipelines-default/\n",
      "gs://qwiklabs-gcp-01-9fd50757f427_cloudbuild/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ENDPOINT = '62df1bdac38e27a1-dot-us-central1.pipelines.googleusercontent.com' # TO DO: REPLACE WITH YOUR ENDPOINT\n",
    "ARTIFACT_STORE_URI = 'gs://qwiklabs-gcp-01-9fd50757f427-kubeflowpipelines-default'  # TO DO: REPLACE WITH YOUR ARTIFACT_STORE NAME \n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='trainer_image'\n",
    "TAG='latest'\n",
    "TRAINER_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 3.4 KiB before compression.\n",
      "Uploading tarball of [trainer_image] to [gs://qwiklabs-gcp-01-9fd50757f427_cloudbuild/source/1625237002.935745-fbe5e4d20389469b9e7708bbc02c11d7.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-01-9fd50757f427/locations/global/builds/6d034289-fdbb-4490-be33-9ff8a37c636b].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/6d034289-fdbb-4490-be33-9ff8a37c636b?project=661185394357].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"6d034289-fdbb-4490-be33-9ff8a37c636b\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-01-9fd50757f427_cloudbuild/source/1625237002.935745-fbe5e4d20389469b9e7708bbc02c11d7.tgz#1625237003315290\n",
      "Copying gs://qwiklabs-gcp-01-9fd50757f427_cloudbuild/source/1625237002.935745-fbe5e4d20389469b9e7708bbc02c11d7.tgz#1625237003315290...\n",
      "/ [1 files][  1.6 KiB/  1.6 KiB]                                                \n",
      "Operation completed over 1 objects/1.6 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.144kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "25fa05cd42bd: Pulling fs layer\n",
      "334312f3cce5: Pulling fs layer\n",
      "69ae64a48940: Pulling fs layer\n",
      "fe5bbb0e39a4: Pulling fs layer\n",
      "fe5bbb0e39a4: Waiting\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "80ddd5a9f1f3: Pulling fs layer\n",
      "76405ca8c0d7: Pulling fs layer\n",
      "96b547bd5851: Pulling fs layer\n",
      "165d356cc142: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "80ddd5a9f1f3: Waiting\n",
      "76405ca8c0d7: Waiting\n",
      "96b547bd5851: Waiting\n",
      "1f14d2578dc7: Pulling fs layer\n",
      "165d356cc142: Waiting\n",
      "1f14d2578dc7: Waiting\n",
      "05c08f7ba2fb: Pulling fs layer\n",
      "b1c9ec7e36f6: Pulling fs layer\n",
      "94e3f0ba3fb3: Pulling fs layer\n",
      "00f212e657e4: Pulling fs layer\n",
      "7d32fad557e1: Pulling fs layer\n",
      "0024a6835488: Pulling fs layer\n",
      "a40f452ef7cf: Pulling fs layer\n",
      "05c08f7ba2fb: Waiting\n",
      "b1c9ec7e36f6: Waiting\n",
      "94e3f0ba3fb3: Waiting\n",
      "00f212e657e4: Waiting\n",
      "7d32fad557e1: Waiting\n",
      "0024a6835488: Waiting\n",
      "a40f452ef7cf: Waiting\n",
      "334312f3cce5: Verifying Checksum\n",
      "334312f3cce5: Download complete\n",
      "25fa05cd42bd: Verifying Checksum\n",
      "25fa05cd42bd: Download complete\n",
      "4f4fb700ef54: Download complete\n",
      "80ddd5a9f1f3: Verifying Checksum\n",
      "80ddd5a9f1f3: Download complete\n",
      "fe5bbb0e39a4: Download complete\n",
      "96b547bd5851: Verifying Checksum\n",
      "96b547bd5851: Download complete\n",
      "165d356cc142: Verifying Checksum\n",
      "165d356cc142: Download complete\n",
      "1f14d2578dc7: Verifying Checksum\n",
      "1f14d2578dc7: Download complete\n",
      "05c08f7ba2fb: Verifying Checksum\n",
      "05c08f7ba2fb: Download complete\n",
      "b1c9ec7e36f6: Verifying Checksum\n",
      "b1c9ec7e36f6: Download complete\n",
      "94e3f0ba3fb3: Verifying Checksum\n",
      "94e3f0ba3fb3: Download complete\n",
      "00f212e657e4: Download complete\n",
      "7d32fad557e1: Verifying Checksum\n",
      "7d32fad557e1: Download complete\n",
      "76405ca8c0d7: Verifying Checksum\n",
      "76405ca8c0d7: Download complete\n",
      "a40f452ef7cf: Verifying Checksum\n",
      "a40f452ef7cf: Download complete\n",
      "25fa05cd42bd: Pull complete\n",
      "69ae64a48940: Verifying Checksum\n",
      "69ae64a48940: Download complete\n",
      "334312f3cce5: Pull complete\n",
      "0024a6835488: Verifying Checksum\n",
      "0024a6835488: Download complete\n",
      "69ae64a48940: Pull complete\n",
      "fe5bbb0e39a4: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "80ddd5a9f1f3: Pull complete\n",
      "76405ca8c0d7: Pull complete\n",
      "96b547bd5851: Pull complete\n",
      "165d356cc142: Pull complete\n",
      "1f14d2578dc7: Pull complete\n",
      "05c08f7ba2fb: Pull complete\n",
      "b1c9ec7e36f6: Pull complete\n",
      "94e3f0ba3fb3: Pull complete\n",
      "00f212e657e4: Pull complete\n",
      "7d32fad557e1: Pull complete\n",
      "0024a6835488: Pull complete\n",
      "a40f452ef7cf: Pull complete\n",
      "Digest: sha256:c3cba94690c7965f6561ebd38eeff7b0cc44b074df925d658afed5d6e9c7cf9d\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 61dc917f58cf\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      " ---> Running in aa073da43078\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas==0.24.2) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=4edff01ea652376f5fb4da6d79901f41867111208c999bba3aee073cd2aafb04\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3988 sha256=baa954054ffb15b75ec375d24f811314a81a6d3fce77d0d3a8f9bb96e9675641\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=0fe813c0e6477846eccb0ef47e628af797de8e49e00feddde0a6471a62cd7b5b\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire cloudml-hypertune termcolor\n",
      "Installing collected packages: termcolor, scikit-learn, pandas, fire, cloudml-hypertune\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.2.4\n",
      "    Uninstalling pandas-1.2.4:\n",
      "      Successfully uninstalled pandas-1.2.4\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "visions 0.7.1 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "phik 0.11.2 requires pandas>=0.25.1, but you have pandas 0.24.2 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "\u001b[0mSuccessfully installed cloudml-hypertune-0.1.0.dev6 fire-0.4.0 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "\u001b[91mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container aa073da43078\n",
      " ---> 2ce76c0ce63c\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in e82f1be7cfd4\n",
      "Removing intermediate container e82f1be7cfd4\n",
      " ---> 7190b4fb9739\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 2dbf20897b39\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 8ff792d78edf\n",
      "Removing intermediate container 8ff792d78edf\n",
      " ---> bc496cee3802\n",
      "Successfully built bc496cee3802\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-01-9fd50757f427/trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-01-9fd50757f427/trainer_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-01-9fd50757f427/trainer_image]\n",
      "6f3ab2e95e3c: Preparing\n",
      "6d4066cd8548: Preparing\n",
      "9db1a6f6c46b: Preparing\n",
      "8ab1d816881d: Preparing\n",
      "dc45e5046eba: Preparing\n",
      "29bf522d97b4: Preparing\n",
      "d96c519f0898: Preparing\n",
      "ff0a6aeeabc0: Preparing\n",
      "8a7cebfdebb3: Preparing\n",
      "c7b3d31e30d8: Preparing\n",
      "2bca48c23a02: Preparing\n",
      "e3772a518bda: Preparing\n",
      "89d9319acee8: Preparing\n",
      "3bcf6b758d7b: Preparing\n",
      "b07bdae450b2: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "844bb7de649e: Preparing\n",
      "e181fed681e9: Preparing\n",
      "e39414beba01: Preparing\n",
      "8f8f0266f834: Preparing\n",
      "29bf522d97b4: Waiting\n",
      "d96c519f0898: Waiting\n",
      "ff0a6aeeabc0: Waiting\n",
      "8a7cebfdebb3: Waiting\n",
      "c7b3d31e30d8: Waiting\n",
      "2bca48c23a02: Waiting\n",
      "e3772a518bda: Waiting\n",
      "89d9319acee8: Waiting\n",
      "3bcf6b758d7b: Waiting\n",
      "b07bdae450b2: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "844bb7de649e: Waiting\n",
      "e181fed681e9: Waiting\n",
      "e39414beba01: Waiting\n",
      "8f8f0266f834: Waiting\n",
      "8ab1d816881d: Layer already exists\n",
      "dc45e5046eba: Layer already exists\n",
      "d96c519f0898: Layer already exists\n",
      "29bf522d97b4: Layer already exists\n",
      "ff0a6aeeabc0: Layer already exists\n",
      "8a7cebfdebb3: Layer already exists\n",
      "2bca48c23a02: Layer already exists\n",
      "c7b3d31e30d8: Layer already exists\n",
      "89d9319acee8: Layer already exists\n",
      "e3772a518bda: Layer already exists\n",
      "3bcf6b758d7b: Layer already exists\n",
      "b07bdae450b2: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "844bb7de649e: Layer already exists\n",
      "e181fed681e9: Layer already exists\n",
      "e39414beba01: Layer already exists\n",
      "8f8f0266f834: Layer already exists\n",
      "6f3ab2e95e3c: Pushed\n",
      "6d4066cd8548: Pushed\n",
      "9db1a6f6c46b: Pushed\n",
      "latest: digest: sha256:c057cfd2a37c055f67f36fbc9dd7f58e705fb7e8246c6e83b971da0deddcebd0 size: 4500\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                       STATUS\n",
      "6d034289-fdbb-4490-be33-9ff8a37c636b  2021-07-02T14:43:23+00:00  2M22S     gs://qwiklabs-gcp-01-9fd50757f427_cloudbuild/source/1625237002.935745-fbe5e4d20389469b9e7708bbc02c11d7.tgz  gcr.io/qwiklabs-gcp-01-9fd50757f427/trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $TRAINER_IMAGE trainer_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='base_image'\n",
    "TAG='latest'\n",
    "BASE_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 244 bytes before compression.\n",
      "Uploading tarball of [base_image] to [gs://qwiklabs-gcp-01-9fd50757f427_cloudbuild/source/1625237258.285619-423f83140f2247bb82044644c3ba63fc.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-01-9fd50757f427/locations/global/builds/44e1a912-62a3-4b51-90bb-e2da38e18acc].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/44e1a912-62a3-4b51-90bb-e2da38e18acc?project=661185394357].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"44e1a912-62a3-4b51-90bb-e2da38e18acc\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-01-9fd50757f427_cloudbuild/source/1625237258.285619-423f83140f2247bb82044644c3ba63fc.tgz#1625237258639472\n",
      "Copying gs://qwiklabs-gcp-01-9fd50757f427_cloudbuild/source/1625237258.285619-423f83140f2247bb82044644c3ba63fc.tgz#1625237258639472...\n",
      "/ [1 files][  284.0 B/  284.0 B]                                                \n",
      "Operation completed over 1 objects/284.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  3.584kB\n",
      "Step 1/2 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "25fa05cd42bd: Pulling fs layer\n",
      "334312f3cce5: Pulling fs layer\n",
      "69ae64a48940: Pulling fs layer\n",
      "fe5bbb0e39a4: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "80ddd5a9f1f3: Pulling fs layer\n",
      "76405ca8c0d7: Pulling fs layer\n",
      "96b547bd5851: Pulling fs layer\n",
      "165d356cc142: Pulling fs layer\n",
      "1f14d2578dc7: Pulling fs layer\n",
      "05c08f7ba2fb: Pulling fs layer\n",
      "b1c9ec7e36f6: Pulling fs layer\n",
      "94e3f0ba3fb3: Pulling fs layer\n",
      "00f212e657e4: Pulling fs layer\n",
      "7d32fad557e1: Pulling fs layer\n",
      "0024a6835488: Pulling fs layer\n",
      "a40f452ef7cf: Pulling fs layer\n",
      "fe5bbb0e39a4: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "80ddd5a9f1f3: Waiting\n",
      "76405ca8c0d7: Waiting\n",
      "96b547bd5851: Waiting\n",
      "165d356cc142: Waiting\n",
      "1f14d2578dc7: Waiting\n",
      "05c08f7ba2fb: Waiting\n",
      "b1c9ec7e36f6: Waiting\n",
      "94e3f0ba3fb3: Waiting\n",
      "00f212e657e4: Waiting\n",
      "7d32fad557e1: Waiting\n",
      "0024a6835488: Waiting\n",
      "a40f452ef7cf: Waiting\n",
      "334312f3cce5: Download complete\n",
      "25fa05cd42bd: Verifying Checksum\n",
      "25fa05cd42bd: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "80ddd5a9f1f3: Verifying Checksum\n",
      "80ddd5a9f1f3: Download complete\n",
      "fe5bbb0e39a4: Verifying Checksum\n",
      "fe5bbb0e39a4: Download complete\n",
      "96b547bd5851: Verifying Checksum\n",
      "96b547bd5851: Download complete\n",
      "165d356cc142: Verifying Checksum\n",
      "165d356cc142: Download complete\n",
      "1f14d2578dc7: Verifying Checksum\n",
      "1f14d2578dc7: Download complete\n",
      "05c08f7ba2fb: Download complete\n",
      "b1c9ec7e36f6: Verifying Checksum\n",
      "b1c9ec7e36f6: Download complete\n",
      "94e3f0ba3fb3: Verifying Checksum\n",
      "94e3f0ba3fb3: Download complete\n",
      "00f212e657e4: Verifying Checksum\n",
      "00f212e657e4: Download complete\n",
      "7d32fad557e1: Verifying Checksum\n",
      "7d32fad557e1: Download complete\n",
      "76405ca8c0d7: Verifying Checksum\n",
      "76405ca8c0d7: Download complete\n",
      "a40f452ef7cf: Verifying Checksum\n",
      "a40f452ef7cf: Download complete\n",
      "69ae64a48940: Verifying Checksum\n",
      "69ae64a48940: Download complete\n",
      "25fa05cd42bd: Pull complete\n",
      "334312f3cce5: Pull complete\n",
      "0024a6835488: Verifying Checksum\n",
      "0024a6835488: Download complete\n",
      "69ae64a48940: Pull complete\n",
      "fe5bbb0e39a4: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "80ddd5a9f1f3: Pull complete\n",
      "76405ca8c0d7: Pull complete\n",
      "96b547bd5851: Pull complete\n",
      "165d356cc142: Pull complete\n",
      "1f14d2578dc7: Pull complete\n",
      "05c08f7ba2fb: Pull complete\n",
      "b1c9ec7e36f6: Pull complete\n",
      "94e3f0ba3fb3: Pull complete\n",
      "00f212e657e4: Pull complete\n",
      "7d32fad557e1: Pull complete\n",
      "0024a6835488: Pull complete\n",
      "a40f452ef7cf: Pull complete\n",
      "Digest: sha256:c3cba94690c7965f6561ebd38eeff7b0cc44b074df925d658afed5d6e9c7cf9d\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 61dc917f58cf\n",
      "Step 2/2 : RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5\n",
      " ---> Running in 8a9d08574d37\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Collecting scikit-learn==0.20.4\n",
      "  Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting pandas==0.24.2\n",
      "  Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Collecting kfp==0.2.5\n",
      "  Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2021.1)\n",
      "Collecting urllib3<1.25,>=1.15\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2021.5.30)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (5.4.1)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.38.0)\n",
      "Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "  Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.1.0)\n",
      "Requirement already satisfied: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (3.4.7)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.30.2)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting cloudpickle==1.1.1\n",
      "  Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "  Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "Collecting argo-models==2.2.1a\n",
      "  Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (3.2.0)\n",
      "Collecting tabulate==0.8.3\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Collecting click==7.0\n",
      "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.2.5) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=2.4.2->kfp==0.2.5) (2.20)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (49.6.0.post20210108)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (0.2.7)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.25.1)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (1.3.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (1.7.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.30.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp==0.2.5) (20.9)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.53.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp==0.2.5) (3.16.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.1.2)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (4.5.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (21.2.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (0.57.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage>=1.13.0->kfp==0.2.5) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==0.2.5) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (4.0.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==0.2.5) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema>=3.0.1->kfp==0.2.5) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema>=3.0.1->kfp==0.2.5) (3.4.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.1.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==0.2.5) (0.36.2)\n",
      "Building wheels for collected packages: kfp, argo-models, tabulate, kfp-server-api, fire, strip-hints, termcolor\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159979 sha256=a74b5f0306a5f1caaf749f779db0a9eb4588ddafaf5a9ed6f639231c024d3858\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/74/7e/0a882d654bdf82d039460ab5c6adf8724ae56e277de7c0eaea\n",
      "  Building wheel for argo-models (setup.py): started\n",
      "  Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "  Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57308 sha256=07360563f7628d4915eee8fb35315a6622b89912638e81cd3ca97295e89f239a\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23379 sha256=7e89fcd269e635e66e903b21e6666a3da45aeea0827ee16bdcafb9e703ea4d47\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102470 sha256=443a210492e0ced297910473e2dfbf838aca7215142692dd5dd66e8433e8dd2a\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/e3/43/3972dea76ee89e35f090b313817089043f2609236cf560069d\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=30bdcaef08fecf4fd19912c17ba0affbd57f5309ec09960e00851fb98ec1e5ab\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=20993 sha256=8a4cf9f2134ec6a09b07ce89b2d70059814346afffd7b7f28ffcebfb9c552a90\n",
      "  Stored in directory: /root/.cache/pip/wheels/2d/b8/4e/a3ec111d2db63cec88121bd7c0ab1a123bce3b55dd19dda5c1\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=5a9cab10d87ea6f306fab615cc5b70bc3bc087df7f16dc8035eb437aebd65283\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built kfp argo-models tabulate kfp-server-api fire strip-hints termcolor\n",
      "Installing collected packages: urllib3, kubernetes, termcolor, tabulate, strip-hints, requests-toolbelt, kfp-server-api, Deprecated, cloudpickle, click, argo-models, scikit-learn, pandas, kfp, fire\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.5\n",
      "    Uninstalling urllib3-1.26.5:\n",
      "      Successfully uninstalled urllib3-1.26.5\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 17.17.0\n",
      "    Uninstalling kubernetes-17.17.0:\n",
      "      Successfully uninstalled kubernetes-17.17.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.6.0\n",
      "    Uninstalling cloudpickle-1.6.0:\n",
      "      Successfully uninstalled cloudpickle-1.6.0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.1\n",
      "    Uninstalling click-8.0.1:\n",
      "      Successfully uninstalled click-8.0.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.2.4\n",
      "    Uninstalling pandas-1.2.4:\n",
      "      Successfully uninstalled pandas-1.2.4\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda 4.9.2 requires ruamel_yaml>=0.11.14, which is not installed.\n",
      "visions 0.7.1 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "phik 0.11.2 requires pandas>=0.25.1, but you have pandas 0.24.2 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "jupyterlab-git 0.11.0 requires nbdime<2.0.0,>=1.1.0, but you have nbdime 3.1.0 which is incompatible.\n",
      "black 21.5b2 requires click>=7.1.2, but you have click 7.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.12 argo-models-2.2.1a0 click-7.0 cloudpickle-1.1.1 fire-0.4.0 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 pandas-0.24.2 requests-toolbelt-0.9.1 scikit-learn-0.20.4 strip-hints-0.1.9 tabulate-0.8.3 termcolor-1.1.0 urllib3-1.24.3\n",
      "\u001b[91mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 8a9d08574d37\n",
      " ---> f848e13abf94\n",
      "Successfully built f848e13abf94\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-01-9fd50757f427/base_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-01-9fd50757f427/base_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-01-9fd50757f427/base_image]\n",
      "945712d34f4b: Preparing\n",
      "8ab1d816881d: Preparing\n",
      "dc45e5046eba: Preparing\n",
      "29bf522d97b4: Preparing\n",
      "d96c519f0898: Preparing\n",
      "ff0a6aeeabc0: Preparing\n",
      "8a7cebfdebb3: Preparing\n",
      "c7b3d31e30d8: Preparing\n",
      "2bca48c23a02: Preparing\n",
      "e3772a518bda: Preparing\n",
      "89d9319acee8: Preparing\n",
      "3bcf6b758d7b: Preparing\n",
      "b07bdae450b2: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "844bb7de649e: Preparing\n",
      "e181fed681e9: Preparing\n",
      "e39414beba01: Preparing\n",
      "8f8f0266f834: Preparing\n",
      "ff0a6aeeabc0: Waiting\n",
      "8a7cebfdebb3: Waiting\n",
      "c7b3d31e30d8: Waiting\n",
      "2bca48c23a02: Waiting\n",
      "e3772a518bda: Waiting\n",
      "89d9319acee8: Waiting\n",
      "3bcf6b758d7b: Waiting\n",
      "b07bdae450b2: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "844bb7de649e: Waiting\n",
      "e181fed681e9: Waiting\n",
      "e39414beba01: Waiting\n",
      "8f8f0266f834: Waiting\n",
      "8ab1d816881d: Layer already exists\n",
      "d96c519f0898: Layer already exists\n",
      "29bf522d97b4: Layer already exists\n",
      "dc45e5046eba: Layer already exists\n",
      "8a7cebfdebb3: Layer already exists\n",
      "c7b3d31e30d8: Layer already exists\n",
      "2bca48c23a02: Layer already exists\n",
      "ff0a6aeeabc0: Layer already exists\n",
      "e3772a518bda: Layer already exists\n",
      "89d9319acee8: Layer already exists\n",
      "b07bdae450b2: Layer already exists\n",
      "3bcf6b758d7b: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "844bb7de649e: Layer already exists\n",
      "e181fed681e9: Layer already exists\n",
      "e39414beba01: Layer already exists\n",
      "8f8f0266f834: Layer already exists\n",
      "945712d34f4b: Pushed\n",
      "latest: digest: sha256:88500937669020208508dd0b5bd89553c3359b8b5d414d0eae4ecf4c970977eb size: 4085\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                    STATUS\n",
      "44e1a912-62a3-4b51-90bb-e2da38e18acc  2021-07-02T14:47:38+00:00  2M31S     gs://qwiklabs-gcp-01-9fd50757f427_cloudbuild/source/1625237258.285619-423f83140f2247bb82044644c3ba63fc.tgz  gcr.io/qwiklabs-gcp-01-9fd50757f427/base_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $BASE_IMAGE base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_KFP_SA = False\n",
    "\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/'\n",
    "RUNTIME_VERSION = '1.15'\n",
    "PYTHON_VERSION = '3.7'\n",
    "\n",
    "%env USE_KFP_SA={USE_KFP_SA}\n",
    "%env BASE_IMAGE={BASE_IMAGE}\n",
    "%env TRAINER_IMAGE={TRAINER_IMAGE}\n",
    "%env COMPONENT_URL_SEARCH_PREFIX={COMPONENT_URL_SEARCH_PREFIX}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERSION={PYTHON_VERSION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py pipeline/covertype_training_pipeline.py --output covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the pipeline package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME='covertype_continuous_training'\n",
    "\n",
    "!kfp --endpoint $ENDPOINT pipeline upload \\\n",
    "-p $PIPELINE_NAME \\\n",
    "covertype_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kfp --endpoint $ENDPOINT pipeline list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID='0918568d-758c-46cf-9752-e04a4403cd84' # TO DO: REPLACE WITH YOUR PIPELINE ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'Covertype_Classifier_Training'\n",
    "RUN_ID = 'Run_001'\n",
    "SOURCE_TABLE = 'covertype_dataset.covertype'\n",
    "DATASET_ID = 'splits'\n",
    "EVALUATION_METRIC = 'accuracy'\n",
    "EVALUATION_METRIC_THRESHOLD = '0.69'\n",
    "MODEL_ID = 'covertype_classifier'\n",
    "VERSION_ID = 'v01'\n",
    "REPLACE_EXISTING_VERSION = 'True'\n",
    "\n",
    "GCS_STAGING_PATH = '{}/staging'.format(ARTIFACT_STORE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kfp --endpoint $ENDPOINT run submit \\\n",
    "-e $EXPERIMENT_NAME \\\n",
    "-r $RUN_ID \\\n",
    "-p $PIPELINE_ID \\\n",
    "project_id=$PROJECT_ID \\\n",
    "gcs_root=$GCS_STAGING_PATH \\\n",
    "region=$REGION \\\n",
    "source_table_name=$SOURCE_TABLE \\\n",
    "dataset_id=$DATASET_ID \\\n",
    "evaluation_metric_name=$EVALUATION_METRIC \\\n",
    "evaluation_metric_threshold=$EVALUATION_METRIC_THRESHOLD \\\n",
    "model_id=$MODEL_ID \\\n",
    "version_id=$VERSION_ID \\\n",
    "replace_existing_version=$REPLACE_EXISTING_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $ENDPOINT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
